\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\begin{document}
\title{Toward real-time data query systems in HEP}

\author{Jim Pivarski}

\address{147 N.\ Ridgeland Ave.\ Unit 3S, Oak Park, IL 60302, US}

\ead{pivarski@fnal.gov}

\begin{abstract}
Exploratory data analysis tools must respond quickly to a user's questions, so that the answer to one question (e.g.\ a visualized histogram or fit) can influence the next. In some SQL-based query systems used in industry, even very large (petabyte) datasets can be summarized on a human timescale (seconds), employing techniques such as columnar data representation, caching, indexing, and code generation/JIT-compilation. This article describes progress toward realizing such a system for High Energy Physics (HEP), focusing on the intermediate problems of optimizing data access and calculations for ``query sized'' payloads, such as a single histogram or group of histograms, rather than large reconstruction or data-skimming jobs. These techniques include direct extraction of ROOT TBranches into Numpy arrays and compilation of Python analysis functions (rather than SQL) to be executed very quickly. We will also discuss the problem of caching and actively delivering jobs to worker nodes that have the necessary input data preloaded in cache. All of these pieces of the larger solution are available as standalone GitHub repositories, and could be used in current analyses.
\end{abstract}

\section{Benefits of a query service}

Despite everything that's changed in computing since the first AIHENP/ACAT workshop, one aspect of today's High Energy Physics (HEP) analyses that a physicist from 27 years ago would recognize is the use of private skims. Rather than analyzing the reconstructed data directly, physicists skim it (dropping events) and slim it (dropping particle attributes), copy this summary closer to where they will be working, and then analyze the summary. The reason is performance: smaller data with fewer steps between the analyst and the data result in quicker exploratory plots, which allow the human analyst to be more engaged in the investigation. Long processing times are acceptable for established, push-button procedures, but a new analysis requires creative work and interactive discovery.

However, producing such a skim can take a long time--- weeks or months before seeing the first plot--- and is fraught with tradeoffs. Excluding important events or (more often) important particle attributes can be costly, and including unnecessary data only makes the process take longer and use more disk. Moreover, the copy uses extra resources (disk and processing), which will only get tighter as data volumes outpace computing budgets, it can effectively ``price out'' small analysis groups. Since it is a distinct copy, it does not benefit from improvements or corrections to the source data until refreshed, and can often be a year or more out of date. Finally, it forces physicists to take time and attention away from physics and statistical issues to solve an IT problem. If the analysis can be performed without this skim, so much the better.

Real-time analysis on primary datasets is possible, and has been demonstrated in industry, usually as large, distributed databases used internally by a company's data analytics team. These systems are built from Apache Drill, Impala, Kudu, Hawk, or Dremel and tuned to provide SQL access to petabytes of data in seconds. Unlike Hadoop and Spark, which are optimized for high throughput, these systems are optimized for low latency, and they use indexes and query planning to avoid churning through all data with every request.

We can learn from these systems, adopting their techniques to provide similar access to HEP data, but there are some important differences between industry needs and HEP needs. Physics analyses are both simpler and more complex than a data scientist's analysis: it is simpler in that HEP does not require any all-to-all processing, such as transposing a table of customer purchases to product correlations to make recommendations--- all of the information that is needed to process one physics event is contained in that event or auxiliary tables. It is not necessary to examine all other events to understand one event. However, physics analyses are more complex in the depth of processing applied to that one event, examining many combinations of particles, searching for correlations. To help with the bookkeeping, physicists are inclined to think in terms of objects, iterating over collections of particle objects, rather than exploding and aggregating tables as in SQL.

Because of the differences, we do not expect an off-the-shelf database to solve our problem, but we can adapt established techniques to work for HEP.

\section{Optimizing for ``query sized'' payloads}

\section{Code transformation and compilation}

\section{Distributed processing with third-party tools}

\section{Advanced query language}

\section{Standalone pieces and the big picture}


\end{document}
